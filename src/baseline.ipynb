{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "import random \n",
    "import collections\n",
    "# use multiprocessing for multicore ops\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"../data/questions.csv\", delimiter=',', encoding=\"utf-8-sig\")\n",
    "data1 = copy.deepcopy(data2[:50000])\n",
    "print(data1.shape)\n",
    "print(data1.iloc[0:1,3:])\n",
    "#random.shuffle(data1)\n",
    "data1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1[[\"question1\",\"question2\"]]\n",
    "y = data1[['is_duplicate']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print(stopwords.words(\"english\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_words( raw_quest, lower=True, remove_not = False ):\n",
    "    \n",
    "    lemmetizer = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_quest)\n",
    "    if lower==True:\n",
    "        letters_only = letters_only.lower()\n",
    "    words = nltk.word_tokenize(letters_only)\n",
    "    #words = nltk.word_tokenize(s)\n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    not_stop = ['no', 'nor', 'not', \"don't\", \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    if remove_not == True:\n",
    "        #stop = stops.difference(not_stop)\n",
    "        stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    meaningful_words = [lemmetizer.lemmatize(w) for w in meaningful_words]\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train['question2']= X_train['question2'].apply(question_to_words)\n",
    "#X_train = X_train.apply(question_to_words)\n",
    "print(X_train.shape)\n",
    "\n",
    "#a = X_train[['question1']]\n",
    "#a = a.apply(question_to_words)\n",
    "X_train['question1'] = X_train['question1'].apply(question_to_words)\n",
    "X_train['question2'] = X_train['question2'].apply(question_to_words)\n",
    "#b=X_train[['question2']]\n",
    "#b.apply(question_to_words)\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "#add whitelisting\n",
    "# use ngrams , ngram_range=(1,2) check for various combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = X_train[\"question1\"] + \" \" + X_train[\"question2\"]\n",
    "corpus = X_train[\"question1\"] + \" \" + X_train[\"question2\"]\n",
    "cv = vectorizer.fit(corpus)\n",
    "\n",
    "cv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data = X_train + y_train\n",
    "new_data = pd.concat([X_train, y_train], axis=1, join_axes=[X_train.index])\n",
    "print(new_data.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = new_data[new_data.is_duplicate==1]\n",
    "nonduplicates = new_data[new_data.is_duplicate==0]\n",
    "print(duplicates.shape)\n",
    "print(nonduplicates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(df):\n",
    "    r,c = df.shape\n",
    "    print(str(r)+ str(c))\n",
    "    final_vec = []\n",
    "    for j in range(r):\n",
    "        a = cv.transform([df.iloc[j].question1])\n",
    "        b = cv.transform([df.iloc[j].question2])\n",
    "        sim = round(cosine_similarity(a,b).ravel()[0], 3)\n",
    "        c = df.iloc[j].is_duplicate\n",
    "        final_vec.append([sim, c])\n",
    "    return final_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = getSimilarity(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(sim)\n",
    "df1.columns = [\"cos_sim\", \"y\"]\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1['cos_sim']\n",
    "y = df1['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf= RandomForestClassifier()\n",
    "X = X.values.reshape(1, -1)\n",
    "y = y.values.reshape(1, -1)\n",
    "clf.fit(X,y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
