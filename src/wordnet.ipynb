{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/shubh24/wordnet-similarity-matrix-a-naive-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(text, lower=False, remove_not=False):\n",
    "    lemmetizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    if lower == True:\n",
    "        text = text.lower()\n",
    "    \n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    not_stop = ['no', 'nor', 'not', \"don't\", \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\",\n",
    "                'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn',\n",
    "                \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "                \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    if remove_not == True:\n",
    "        # stop = stops.difference(not_stop)\n",
    "        stops = ['the', 'a', 'an', 'and', 'but', 'if', 'or', 'because', 'as', 'what', 'which', 'this', 'that', 'these',\n",
    "                 'those', 'then',\n",
    "                 'just', 'so', 'than', 'such', 'both', 'through', 'about', 'for', 'is', 'of', 'while', 'during', 'to',\n",
    "                 'What', 'Which',\n",
    "                 'Is', 'If', 'While', 'This']\n",
    "    \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "\n",
    "    meaningful_words = [lemmetizer.lemmatize(w) for w in meaningful_words]\n",
    "    \n",
    "    return meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass a row and get pair score\n",
    "def get_pair_score(row):\n",
    "    res = row[\"is_duplicate\"]\n",
    "    terms1 = get_terms(row[\"question1\"])\n",
    "    terms2 = get_terms(row[\"question2\"])\n",
    "\n",
    "    sims = []\n",
    "    \n",
    "    for word1 in terms1:\n",
    "        word1_sim = []\n",
    "\n",
    "        try:\n",
    "            syn1 = wn.synsets(word1)[0]\n",
    "        except:  #if wordnet is not able to find a synset for word1\n",
    "            sims.append([0 for i in range(0, len(terms2))])\n",
    "            continue\n",
    "\n",
    "\n",
    "        for word2 in terms2:\n",
    "            try:\n",
    "                syn2 = wn.synsets(word2)[0]\n",
    "            except: #if wordnet is not able to find a synset for word2\n",
    "                word1_sim.append(0)\n",
    "                continue\n",
    "\n",
    "            word_similarity = syn1.wup_similarity(syn2)\n",
    "            word1_sim.append(word_similarity)\n",
    "\n",
    "        sims.append(word1_sim)\n",
    "        \n",
    "        \n",
    "    if(len(terms1)==0 or len(terms2)==0):\n",
    "        return 0\n",
    "        \n",
    "    word1_score = 0\n",
    "    for i in range(0, len(terms1), 1):\n",
    "        try:\n",
    "            word1_score += max(sims[i])\n",
    "        except:\n",
    "            continue\n",
    "    word1_score /= len(terms1) #Averaging over all terms\n",
    "    \n",
    "    \n",
    "    word2_score = 0\n",
    "    for i in range(0, len(terms2), 1):\n",
    "        try:\n",
    "            word2_score += max([j[i] for j in sims])\n",
    "        except:\n",
    "            continue\n",
    "    word2_score /= len(terms2)\n",
    "    \n",
    "    \n",
    "    pair_score = (word1_score + word2_score)/2\n",
    "    \n",
    "    return pair_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_res = []\n",
    "\n",
    "for j in range(50000):\n",
    "    pair_score = get_pair_score(df_train.iloc[j])\n",
    "    \n",
    "    score_res.append([pair_score, df_train.iloc[j]['is_duplicate']])\n",
    "    \n",
    "    \n",
    "score_res_df = pd.DataFrame(score_res, columns = ['pair_score', 'is_duplicate']) \n",
    "# score_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24032538267832385"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = score_res_df.loc[score_res_df.is_duplicate==1]\n",
    "nondupes = score_res_df.loc[score_res_df.is_duplicate==0]\n",
    "\n",
    "median_nondup = np.median(nondupes['pair_score'])\n",
    "median_nondup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.325"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_dup = np.median(duplicates['pair_score'])\n",
    "median_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test\n",
    "results_df = []\n",
    "\n",
    "for j in range(50000,100000):\n",
    "    pair_score = get_pair_score(df_train.iloc[j])\n",
    "    decision = (median_nondup + median_dup)/2\n",
    "    \n",
    "    if(pair_score < decision):\n",
    "        d_bin = 0\n",
    "    else:\n",
    "        d_bin = 1\n",
    "        \n",
    "    results_df.append([d_bin, df_train.iloc[j]['is_duplicate']])\n",
    "                       \n",
    "\n",
    "results_df = pd.DataFrame(results_df, columns = ['dup_bin', 'is_duplicate'])\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.514053965351525\n",
      "accu 0.58542\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "                                 \n",
    "print(\"f1 \" + str(f1_score(results_df['is_duplicate'], results_df['dup_bin'])))\n",
    "print(\"accu \" + str(accuracy_score(results_df['is_duplicate'], results_df['dup_bin'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_train.iloc[0] #Just taking the first row, you can put a loopover \n",
    "\n",
    "res = row[\"is_duplicate\"]\n",
    "terms1 = get_terms(row[\"question1\"])\n",
    "terms2 = get_terms(row[\"question2\"])\n",
    "\n",
    "sims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word1 in terms1:\n",
    "    word1_sim = []\n",
    "\n",
    "    try:\n",
    "        syn1 = wn.synsets(word1)[0]\n",
    "    except:  #if wordnet is not able to find a synset for word1\n",
    "        sims.append([0 for i in range(0, len(terms2))])\n",
    "        continue\n",
    "\n",
    "\n",
    "    for word2 in terms2:\n",
    "        try:\n",
    "            syn2 = wn.synsets(word2)[0]\n",
    "        except: #if wordnet is not able to find a synset for word2\n",
    "            word1_sim.append(0)\n",
    "            continue\n",
    "\n",
    "        word_similarity = syn1.wup_similarity(syn2)\n",
    "        word1_sim.append(word_similarity)\n",
    "\n",
    "    sims.append(word1_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_score = 0\n",
    "for i in range(0, len(terms1), 1):\n",
    "    try:\n",
    "        word1_score += max(sims[i])\n",
    "    except:\n",
    "        continue\n",
    "word1_score /= len(terms1) #Averaging over all terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2_score = 0\n",
    "\n",
    "for i in range(0, len(terms2), 1):\n",
    "    try:\n",
    "        word2_score += max([j[i] for j in sims])\n",
    "    except:\n",
    "        continue\n",
    "word2_score /= len(terms2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40476190476190477"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_score = (word1_score + word2_score)/2\n",
    "pair_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
