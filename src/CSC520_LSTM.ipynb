{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2IsBi3iHI5b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "import random \n",
    "import collections\n",
    "from gensim.models import KeyedVectors\n",
    "from time import time\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import datetime\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda, CuDNNLSTM\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard\n",
    "embedding_dim = 300\n",
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJWwnTTNHOOJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sso37cFMHONE"
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"/gdrive/My Drive/train.csv\", delimiter=',', encoding=\"utf-8-sig\")\n",
    "data2 = pd.read_csv(\"/gdrive/My Drive/test.csv\", delimiter=',', encoding=\"utf-8-sig\")\n",
    "EMBEDDING_FILE = '/gdrive/My Drive/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "print(data1.shape)\n",
    "print(data1.iloc[0:1,3:])\n",
    "#random.shuffle(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5O3ykubHI5g"
   },
   "outputs": [],
   "source": [
    "subset = data1.iloc[0:100,3:]\n",
    "subset\n",
    "#subset = subset.loc[subset.is_duplicate==0,['question1','question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xo2ixcvKHI5i"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5v6a0XyHI5m"
   },
   "outputs": [],
   "source": [
    "def question_to_words( raw_quest ):\n",
    "    lemmetizer = nltk.WordNetLemmatizer()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_quest) \n",
    "    letters_only = letters_only.lower()\n",
    "    words = nltk.word_tokenize(letters_only)\n",
    "    #words = nltk.word_tokenize(s)\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    meaningful_words = [lemmetizer.lemmatize(w) for w in meaningful_words]\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHYfDkb_HI5q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "subset['question1']= subset['question1'].apply(question_to_words)\n",
    "subset['question2']= subset['question2'].apply(question_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQR3F3kLHI5v"
   },
   "outputs": [],
   "source": [
    "X = subset.iloc[:,0:-1]\n",
    "X\n",
    "y = subset.iloc[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JhR1mUleHI5z"
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text): #Standard code used for cleaning data.\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BgX4NZtHI51"
   },
   "outputs": [],
   "source": [
    "\n",
    "vocabulary = dict()\n",
    "vocabulary_T = ['<unk>']  \n",
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Run both traing and test data sets.\n",
    "for dataset in [data1, data2]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        #Use both the questions in 'x' row\n",
    "        for question in questions_cols:\n",
    "\n",
    "            qc = []  \n",
    "            for word in text_to_word_list(row[question]):\n",
    "\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(vocabulary_T)\n",
    "                    qc.append(len(vocabulary_T))\n",
    "                    vocabulary_T.append(word)\n",
    "                else:\n",
    "                    qc.append(vocabulary[word])\n",
    "\n",
    "            dataset.set_value(index, question, qc)\n",
    "            \n",
    "\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0  # To ignore padding\n",
    "\n",
    "# Matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bY9q-42YHI54"
   },
   "outputs": [],
   "source": [
    "X = data1[questions_cols]\n",
    "\n",
    "Y = data1['is_duplicate']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2) # validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Yul-eNyHI57"
   },
   "outputs": [],
   "source": [
    "max_seq_length = max(X_train.question1.map(lambda x: len(x)).max(),\n",
    "                     X_train.question2.map(lambda x: len(x)).max(),\n",
    "                     X_test.question2.map(lambda x: len(x)).max(),\n",
    "                    X_test.question2.map(lambda x: len(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Ly2NosvHI5_"
   },
   "outputs": [],
   "source": [
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_test = {'left': X_test.question1, 'right': X_test.question2}\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "for dataset, side in itertools.product([X_train, X_test], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7eTbvDAdHI6D"
   },
   "outputs": [],
   "source": [
    "\n",
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 10\n",
    "\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tkz5P1nDHI6G"
   },
   "outputs": [],
   "source": [
    "#For TensorBoard\n",
    "import time\n",
    "log_dir=\"/gdrive/My Drive/logs/{}\".format(time.strftime(\"%Y%m%d-%H%M%S\", time.gmtime()))\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T7md7BlrHI6J"
   },
   "outputs": [],
   "source": [
    "# To get f1 score\n",
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tIQDiCrHI6L"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# left right inputs feeded to embedding\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# *****Using GPU*******\n",
    "shared_lstm = CuDNNLSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Lambda Function for manhattan distance\n",
    "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "# Main Model Ready\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "\n",
    "# Adadelta optimizer\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy',f1])\n",
    "\n",
    "\n",
    "malstm_trained = malstm.fit([X_train['left'], X_train['right']], y_train, batch_size=batch_size, nb_epoch=1,validation_data=([X_test['left'], X_test['right']], y_test), callbacks = [tensorboard])\n",
    "print(malstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVOvRtohHI6O"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(malstm, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "from IPython.display import Image\n",
    "Image(retina=True, filename='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGwy1rNKAGE4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig1=plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)\n",
    "fig1.savefig('accuracy.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJ7k2h72AGE0"
   },
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    "fig2.savefig('loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QOI_NBCAGEu"
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure()\n",
    "plt.plot(history.history['f1'],'r',linewidth=3.0)\n",
    "plt.plot(xhistory.history['val_f1'],'b',linewidth=3.0)\n",
    "plt.legend(['F1 Training', 'F1 Validation'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('F1',fontsize=16)\n",
    "plt.title('F1 Curves',fontsize=16)\n",
    "fig3.savefig('f1.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "read_data_and_play_unknown_cause.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
