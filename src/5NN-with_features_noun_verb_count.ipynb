{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "# use multiprocessing for multicore ops\n",
    "import copy\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def noun_count(q1, q2):\n",
    "    s1 = pos_tag(nltk.word_tokenize(q1))\n",
    "    s2 = pos_tag(nltk.word_tokenize(q2))\n",
    "    n1 = []\n",
    "    n2 = []\n",
    "    for i,j in s1:\n",
    "        if j == 'NN':\n",
    "            n1.append(i)\n",
    "\n",
    "    for i,j in s2:\n",
    "        if j == 'NN':\n",
    "            n2.append(i)\n",
    "\n",
    "    s11= set(n1)\n",
    "    s22 = set(n2)\n",
    "    return len(s11.intersection(s22))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_count(q1, q2):\n",
    "    s1 = pos_tag(nltk.word_tokenize(q1))\n",
    "    s2 = pos_tag(nltk.word_tokenize(q2))\n",
    "    n1 = []\n",
    "    n2 = []\n",
    "    for i,j in s1:\n",
    "        if j == 'VB':\n",
    "            n1.append(i)\n",
    "\n",
    "    for i,j in s2:\n",
    "        if j == 'VB':\n",
    "            n2.append(i)\n",
    "\n",
    "    s11= set(n1)\n",
    "    s22 = set(n2)\n",
    "    return len(s11.intersection(s22))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(df, cv):\n",
    "    r,c = df.shape\n",
    "    #print(df)\n",
    "    #print(str(r)+ str(c))\n",
    "    final_vec = []\n",
    "    for j in range(r):\n",
    "        a = cv.transform([df.iloc[j].question1])\n",
    "        b = cv.transform([df.iloc[j].question2])\n",
    "        sim = round(cosine_similarity(a,b).ravel()[0], 3)\n",
    "        nc = noun_count(df.iloc[j].question1,df.iloc[j].question2)\n",
    "        #vc = verb_count(df.iloc[j].question1,df.iloc[j].question2)\n",
    "        #print(nc)\n",
    "        c = df.iloc[j].is_duplicate\n",
    "        final_vec.append([sim, nc, c])\n",
    "    return final_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_words(raw_quest, lower=True, remove_not=True):\n",
    "    lemmetizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_quest)\n",
    "    if lower == True:\n",
    "        letters_only = letters_only.lower()\n",
    "    words = nltk.word_tokenize(letters_only)\n",
    "    # words = nltk.word_tokenize(s)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    not_stop = ['no', 'nor', 'not', \"don't\", \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\",\n",
    "                'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn',\n",
    "                \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "                \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    if remove_not == True:\n",
    "        # stop = stops.difference(not_stop)\n",
    "        stops = ['the', 'a', 'an', 'and', 'but', 'if', 'or', 'because', 'as', 'what', 'which', 'this', 'that', 'these',\n",
    "                 'those', 'then',\n",
    "                 'just', 'so', 'than', 'such', 'both', 'through', 'about', 'for', 'is', 'of', 'while', 'during', 'to',\n",
    "                 'What', 'Which',\n",
    "                 'Is', 'If', 'While', 'This']\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "\n",
    "    meaningful_words = [lemmetizer.lemmatize(w) for w in meaningful_words]\n",
    "    return (\" \".join(meaningful_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "\n",
    "def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n",
    "    # Clean the text, with the option to remove stop_words and to stem words.\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_df(X_train, y_train):\n",
    "    X_train['question1'] = X_train['question1'].apply(text_to_wordlist)\n",
    "    X_train['question2'] = X_train['question2'].apply(text_to_wordlist)\n",
    "    #print(X_train.shape)\n",
    "    #print(X_train.head)\n",
    "\n",
    "    #print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", \\\n",
    "                                 tokenizer=None, \\\n",
    "                                 preprocessor=None, \\\n",
    "                                 stop_words=None, \\\n",
    "                                 ngram_range=(1,2),\\\n",
    "                                 max_features=5000)\n",
    "    corpus = X_train[\"question1\"] + \" \" + X_train[\"question2\"]\n",
    "    cv = vectorizer.fit(corpus)\n",
    "\n",
    "    # cv\n",
    "    #print(X_train)\n",
    "    #print(y_train)\n",
    "    new_data = pd.concat([X_train, y_train], axis=1, join_axes=[X_train.index])\n",
    "    #print(new_data.head)\n",
    "    sim = getSimilarity(new_data, cv)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data2 = pd.read_csv(\"data/train.csv\", delimiter=',', encoding=\"utf-8-sig\")\n",
    "data1 = copy.deepcopy(data2[:50000])\n",
    "data1.dropna(inplace=True)\n",
    "#print(data1.shape)\n",
    "#print(data1.iloc[0:1, 3:])\n",
    "\n",
    "X = data1[[\"question1\", \"question2\"]]\n",
    "y = data1[['is_duplicate']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "#print(stopwords.words(\"english\"))\n",
    "\n",
    "#print(X_train.shape)\n",
    "\n",
    "sim = get_cosine_df(X_train, y_train)\n",
    "df1 = pd.DataFrame(sim)\n",
    "#df1.columns = [\"cos_sim\",\"noun_count\",\"verb_count\", \"y\"]\n",
    "df1.columns = [\"cos_sim\",\"noun_count\", \"y\"]\n",
    "#df1\n",
    "\n",
    "\n",
    "#df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df1[['cos_sim','noun_count', 'verb_count']]\n",
    "X = df1[['cos_sim','noun_count']]\n",
    "#print(X)\n",
    "y = df1['y']\n",
    "#X = X.values.reshape(-1, 1)\n",
    "#y = y.values.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "# neigh.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svm = SVC(gamma='auto')\n",
    "clf_svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16500, 2)\n",
      "(16500, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "sim = get_cosine_df(X_test, y_test)\n",
    "df1 = pd.DataFrame(sim)\n",
    "print(df1.shape)\n",
    "#df1.columns = [\"cos_sim\",\"noun_count\", 'verb_count',\"y\"]\n",
    "df1.columns = [\"cos_sim\",\"noun_count\",\"y\"]\n",
    "#df1\n",
    "#print(df1.head)\n",
    "Xt = df1[['cos_sim','noun_count',]]\n",
    "yt = df1[\"y\"]\n",
    "#Xt = Xt.values.reshape(-1, 1)\n",
    "p = neigh.predict(Xt)\n",
    "# p1 = clf_svm.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    }
   ],
   "source": [
    "n = p.tolist()\n",
    "print(len(n))\n",
    "#n\n",
    "# n1 = p1.tolist()\n",
    "#print(len(n1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    }
   ],
   "source": [
    "fact = yt.tolist()\n",
    "print(len(fact))\n",
    "#fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10294\n",
      "Accuracy = 0.6238787878787879\n",
      "f1 0.5005633349428616\n",
      "accu 0.6238787878787879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "assert(len(fact)==len(p))\n",
    "count = 0\n",
    "for i in range(len(fact)):\n",
    "    if fact[i]==p[i]:\n",
    "        count+=1\n",
    "print(count)\n",
    "print(\"Accuracy = \" + str(count/len(fact)) )\n",
    "\n",
    "print(\"f1 \" + str(f1_score(fact, p)))\n",
    "print(\"accu \" + str(accuracy_score(fact, p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert(len(fact)==len(p1))\n",
    "# count = 0\n",
    "# for i in range(len(fact)):\n",
    "#     if fact[i]==p1[i]:\n",
    "#         count+=1\n",
    "# print(count)\n",
    "# print(\"Accuracy = \" + str(count/len(fact)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
