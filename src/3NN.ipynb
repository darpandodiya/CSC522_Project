{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ss/.local/lib/python3.5/site-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ss/.local/lib/python3.5/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "# use multiprocessing for multicore ops\n",
    "import copy\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def getSimilarity(df, cv):\n",
    "    r,c = df.shape\n",
    "    #print(str(r)+ str(c))\n",
    "    final_vec = []\n",
    "    for j in range(r):\n",
    "        a = cv.transform([df.iloc[j].question1])\n",
    "        b = cv.transform([df.iloc[j].question2])\n",
    "        sim = round(cosine_similarity(a,b).ravel()[0], 3)\n",
    "        c = df.iloc[j].is_duplicate\n",
    "        final_vec.append([sim, c])\n",
    "    return final_vec\n",
    "\n",
    "def question_to_words(raw_quest, lower=True, remove_not=True):\n",
    "    lemmetizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_quest)\n",
    "    if lower == True:\n",
    "        letters_only = letters_only.lower()\n",
    "    words = nltk.word_tokenize(letters_only)\n",
    "    # words = nltk.word_tokenize(s)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    not_stop = ['no', 'nor', 'not', \"don't\", \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\",\n",
    "                'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn',\n",
    "                \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "                \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    if remove_not == True:\n",
    "        # stop = stops.difference(not_stop)\n",
    "        stops = ['the', 'a', 'an', 'and', 'but', 'if', 'or', 'because', 'as', 'what', 'which', 'this', 'that', 'these',\n",
    "                 'those', 'then',\n",
    "                 'just', 'so', 'than', 'such', 'both', 'through', 'about', 'for', 'is', 'of', 'while', 'during', 'to',\n",
    "                 'What', 'Which',\n",
    "                 'Is', 'If', 'While', 'This']\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "\n",
    "    meaningful_words = [lemmetizer.lemmatize(w) for w in meaningful_words]\n",
    "    return (\" \".join(meaningful_words))\n",
    "\n",
    "\n",
    "def get_cosine_df(X_train, y_train):\n",
    "    X_train['question1'] = X_train['question1'].apply(question_to_words)\n",
    "    X_train['question2'] = X_train['question2'].apply(question_to_words)\n",
    "    #print(X_train.shape)\n",
    "    #print(X_train.head)\n",
    "\n",
    "    #print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", \\\n",
    "                                 tokenizer=None, \\\n",
    "                                 preprocessor=None, \\\n",
    "                                 stop_words=None, \\\n",
    "                                 ngram_range=(1,2),\\\n",
    "                                 max_features=5000)\n",
    "    corpus = X_train[\"question1\"] + \" \" + X_train[\"question2\"]\n",
    "    cv = vectorizer.fit(corpus)\n",
    "\n",
    "    # cv\n",
    "    #print(X_train)\n",
    "    #print(y_train)\n",
    "    new_data = pd.concat([X_train, y_train], axis=1, join_axes=[X_train.index])\n",
    "    #print(new_data.head)\n",
    "\n",
    "    duplicates = new_data[new_data.is_duplicate == 1]\n",
    "    nonduplicates = new_data[new_data.is_duplicate == 0]\n",
    "    #print(duplicates.shape)\n",
    "    #print(nonduplicates.shape)\n",
    "\n",
    "    sim = getSimilarity(new_data, cv)\n",
    "    return sim\n",
    "\n",
    "\n",
    "data2 = pd.read_csv(\"../data/questions.csv\", delimiter=',', encoding=\"utf-8-sig\")\n",
    "data1 = copy.deepcopy(data2[:50000])\n",
    "data1.dropna(inplace=True)\n",
    "#print(data1.shape)\n",
    "#print(data1.iloc[0:1, 3:])\n",
    "\n",
    "X = data1[[\"question1\", \"question2\"]]\n",
    "y = data1[['is_duplicate']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "#print(stopwords.words(\"english\"))\n",
    "\n",
    "#print(X_train.shape)\n",
    "\n",
    "sim = get_cosine_df(X_train, y_train)\n",
    "df1 = pd.DataFrame(sim)\n",
    "df1.columns = [\"cos_sim\", \"y\"]\n",
    "#df1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ss/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df1['cos_sim']\n",
    "y = df1['y']\n",
    "X = X.values.reshape(-1, 1)\n",
    "y = y.values.reshape(-1,1)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "sim = get_cosine_df(X_test, y_test)\n",
    "df1 = pd.DataFrame(sim)\n",
    "df1.columns = [\"cos_sim\", \"y\"]\n",
    "#df1\n",
    "#print(df1.head)\n",
    "Xt = df1['cos_sim']\n",
    "yt = df1['y']\n",
    "Xt = Xt.values.reshape(-1, 1)\n",
    "p = neigh.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    }
   ],
   "source": [
    "n = p.tolist()\n",
    "print(len(n))\n",
    "#n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n"
     ]
    }
   ],
   "source": [
    "fact = yt.tolist()\n",
    "print(len(fact))\n",
    "#fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10399\n",
      "Accuracy = 0.6302424242424243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert(len(fact)==len(p))\n",
    "count = 0\n",
    "for i in range(len(fact)):\n",
    "    if fact[i]==p[i]:\n",
    "        count+=1\n",
    "print(count)\n",
    "print(\"Accuracy = \" + str(count/len(fact)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
